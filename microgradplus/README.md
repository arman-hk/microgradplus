#### [engine.py](/microgradplus/engine.py)
- [x] `_grad_fn`: to store the gradient calculation logic specific to the operation that produced the `Value`
- [x] `__repr__`: repr operator to display the data and it's gradient when printed
- [x] `__add__`: addition operator with it's grad func(works on scalars, and arrays)
- [x] `Context`: stores the arrays for backpropagation
- [x] `backward`: backward pass of the computation, calculating gradients for all nodes in the computation graph by chain rule
- [x] `__mul__`: element-wise multiplication operator with it's grad func(works on scalars, and arrays)
- [x] `__pow__`: fast power function with it's grad func(works on scalars, and arrays)
- [x] `__neg__`: negation operator with it's grad func(works on scalars, and arrays)
- [x] `__sub__`: subtraction operator with it's grad func(works on scalars, and arrays)
- [x] `__truediv__`: true division operator with it's grad func(works on scalars, and arrays)
- [x] `sqrt`: square root function with it's grad func(works on scalars, and arrays)
- [x] `exp`: exponential function with it's grad func(works on scalars, and arrays)
- [x] `log`: natural logarithm function with it's grad func(works on scalars, and arrays)
- [x] `abs`: absolute function with it's grad func(works on scalars, and arrays)
- [x] `relu`: rectified linear unit function with it's grad func(works on scalars, and arrays)
- [x] `sigmoid`: element-wise sigmoid function with it's grad func(works on scalars, and arrays)
- [x] `tanh`: hyperbolic tangent function with it's grad func(works on scalars, and arrays)
- [x] `__matmul__`: matrix multiplication @ function with it's grad func(works on arrays)
- [x] `__rmatmul__`: handles the case when the `Value` instance is on the right side of the matmul operation
- [x] `T`: matrix transpose function
- [x] `mse`: mean squared error between the predicted outputs of the neural network and the actual target values
- [x] `mse`: mean absolute error between the predicted outputs of the neural network and the actual target values

#### [nn.py](/microgradplus/nn.py)
- [x] `Linear`: a linear layer in a neural network. It has a forward pass that computes the dot product of the inputs with the weights and adds the bias, and a backward pass that computes the gradients with respect to the inputs and parameters.
- [x] `ReLU`: ReLU activation based on the `relu` function in engine.
- [x] `Tanh`: Tanh activation based on the `tanh` function in engine.
- [x] `MSE`: MSE loss based on the `mse` function in engine.
- [x] `MAE`: MAE loss based on the `mae` function in engine.
- [x] `SGD`: Stochastic gradient descent (SGD) optimzation algorithm.
- [x] `Sequential`: an ordered container of neural network layers. It simplifies the process of defining and training a neural network by providing forward and backward passes through all the included layers.